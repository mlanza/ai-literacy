## 🪄 Magic Moments

We just picked a flight.

That in itself isn’t magic. We gave the AI a list of options, we handed it a traveler profile — our preferences, priorities, tradeoffs — and we asked it to make a call. It did. And what’s wild is that it often makes the *right* call. Not just one that ticks boxes, but one that feels like the same decision we would’ve made ourselves.

That’s the marvel. Not that the model did something complex. But that it understood what we meant.

And it did that *because* we gave it the right material. A structured profile. A set of candidates. A clear ask.

But here’s what’s easy to miss:

What the AI did wasn’t thinking — not in any human sense. It was **textual reasoning**. We codified our values in English, and the model responded by trying to continue the story — not just any story, but *our* story, shaped by our preferences.

And that’s worth sitting with.

## Packaging Your Brand

The amazing piece is the traveler profile — a process artifact.

What’s so surprising isn’t that the model generated a good flight recommendation. It’s that it *understood us*. We didn’t feed it code or rules. We fed it English. A small, structured note written by a human — listing tradeoffs, flagging edge cases, capturing the feel of what matters. And the AI read it. Absorbed it. Applied it. The flight it chose wasn’t just a box-checker. It was the one *we* would’ve picked, had we been the ones doing the search.

That alone is remarkable.

But zoom out. That was just one artifact, for one decision, in one transaction. And it worked.

Now consider this: almost every business already does this kind of encoding. That’s what training manuals are. That’s what ops guides, brand books, and customer service scripts are for. They’re how you capture behavior. Preserve tone. Reproduce experience.

Because when you walk into a Chick-fil-A, you’re not just getting chicken. You’re getting *consistency*.

You’re getting the brand.

And that brand doesn’t emerge from vibe. It comes from artifacts. Training documents. Onboarding modules. Little cues and scripts that teach employees how to behave, how to respond, how to stay on message.

The fact that you hear “my pleasure” instead of “no problem” is no accident. It’s engineered. And it works — not because the people are guessing, but because the playbook told them how to show up.

Now look again at what just happened with the AI.

We gave it a playbook — a traveler profile. And it followed it.

That’s the marvel. We trained the model with nothing but words. No code. No integration. Just clear writing.

Let that sink in.

In a world where more and more of the work is digital — AI lives where that work happens. And that means we can start doing with AI what we’ve always done with humans: *train it*. Not by rewriting the model. Not by fine-tuning. But by handing it documents that capture what matters, when it matters.

So the job of the business leader — or the domain expert — starts to shift.

You’re not writing programs. You’re writing *guides*. Process artifacts. Structured, readable, reusable scaffolds for decision-making.

Because if you can explain your work clearly on paper, you’re not far from training an AI to do it — not in some generic way, but *your* way.

On-brand. On-voice. On-point.

The question isn’t “can AI do my job?”

The question is: *have you written down how you do it, in a way it can read?*

If you have, the model can reason.

If you haven’t, it’s guessing.

And the real shift is this:

AI doesn’t need magic. It needs materials.

Artifacts are the packaging of your judgment.

They are how you train a virtual — and every time the bead drops, the virtual springs into action, reasoning through what it must do to carry the work forward.

And “reasoning” might sound lofty, but it’s really just storytelling. The model opens its mouth and starts talking — not because it knows where it’s headed, but because the words keep coming. One after another, in sequence, shaped only by what came before.

That’s not intelligence. It’s prediction.

Let’s look closer at what that means.

## 🔮 LLMs Are Just Prediction Machines

Large language models don’t plan. They don’t remember what they said. They don’t know what they’re going to say next.

They work one token at a time.

Each word they write is the result of a huge probabilistic computation that asks: *What’s the most likely next word, given everything I’ve seen so far?*

That’s it.

It’s a rolling simulation. A chain reaction. A long string of dominos falling — only instead of knocking over real dominos, each one is generated in the moment. There’s no preset layout. The model computes each next piece from scratch, based on what’s already been laid down.

No awareness. No hindsight. No foresight. Just one word at a time, trying to make the continuation feel plausible.

And what’s astonishing is how often that chain reaction produces something useful.

## 🎲 The Illusion of Non-Determinism

This is where things get slippery.

People assume the model is creative. That it’s riffing. Improvising. And it is — but only because the **sampling layer** *allows* it to be.

The core model itself is deterministic. Give it the same prompt, with the same weights, the same temperature, and the same random seed? You get the exact same output. Every time.

That’s not an accident. That’s baked into the math.

But when tools like ChatGPT introduce temperature — a way of softening the model’s certainty and encouraging it to sample from a wider range of plausible options — you start to see variation. You get different runs. Sometimes better. Sometimes worse.

But under the hood? It’s all still probability. You’re just deciding how much randomness to allow in the selection.

## 🧪 The Hungry… What?

Let’s ground that in a real example.

Imagine you start a story with:

> “Once upon a time, there was a very hungry…”

Internally, the model might assign these probabilities to the next word:

* `"dragon"` → 34%
* `"cat"` → 27%
* `"child"` → 15%
* `"alien"` → 8%
* everything else → 16%

If you set **temperature = 0** (i.e., greedy mode), the model always picks the top token. `"dragon"` wins every time.

If you set **temperature = 0.9** and **top\_p = 0.9**, the model might pick `"cat"` or `"child"` or `"dragon"` — depending on the random draw.

Each word, each token, is another domino. Another decision. Another “next most likely continuation.”

And this is how it always works. Not just for stories. But for picking flights. Writing itineraries. Filling out forms. Responding to users. Continuing a policy doc. Anything.

The output is *always* a probabilistic best guess.

The fact that it sounds smart? Feels aligned? Matches your values?

That’s the miracle.

## 👻 The Hallucination Is the Rule — Not the Exception

People are bothered when they experience hallucinations. The model confidently offers up what they know to be a falsehood.

But here's the thing. The model doesn't sometimes hallucinate.  **It always hallucinates.** That is, it never actually knows whether anything it's saying is true because there's no actual thought behind it.  Everything is utter fabrication based on probability, not fact.

And yet, when we give it the right materials — structured profiles, clean data, clear instructions — we often get outputs that feel thoughtful. On brand. Aligned with our intent.

That’s not luck. It’s weight.

When you feed a model a strong payload — one with crisp framing, rich constraints, and specific examples — you’re not giving it magic. You’re giving it *bias*. You're tilting the probability landscape in favor of something useful. That’s what makes context engineering feel like “training” — even though nothing in the model actually changes.

And this is where the real mental model lives:

**The prompt *is* the system.**

The idea that you can write a traveler profile — or a team playbook, or a guide for responding to customer complaints — and then expect a text generator to *act like it read it* is otherworldly. But it’s not nonsense. That’s what the model was built to do: find continuation patterns that feel plausible in light of what came before.

So the closer your input resembles the right *prior*, the more likely the model will simulate the output you hoped for.

We don’t fully understand why some of this works. The term *emergent behavior* gets thrown around a lot, but:

**The better you frame the input, the better your odds of getting something that makes sense — and is useful.**

That’s not a guarantee. But it’s not random either.

So don’t think of artifacts as “add-ons.” Think of them as *levers*. They don’t just help the model write better. They change the shape of what the model is likely to say next.

## 🧠 Brains in Jars

So when we say the model “understood the traveler profile,” what we mean is this:

It processed the text. It absorbed the constraints. And then it generated a continuation — one that aligned with those constraints, not because it reasoned about them the way we do, but because it had seen *so many* examples of similar tradeoffs in its training data.

It doesn’t think. It performs.

But the performance is good enough that, with the right scaffolding, we can trust it to take a swing at work we’d normally reserve for humans.

It’s a brain in a jar. No hands. No agency. But ready to help.

So when we gave it that profile and the flight options, and asked it to choose the best one? It didn’t guess. It computed the next domino. It continued the story — our story — one word at a time.

That’s not AI *solving* the problem.

It’s AI *simulating* someone solving the problem — and often getting it right.

## ✈️ Bead vs. Baton

But this is only half the story.

Picking a flight is useful. But not amazing. Why? Because it’s still static. Still stuck in the artifact layer.

Flight data changes minute by minute. If the model picked a great option, but the price jumped two hours later, that work has already gone stale.

The real power — the next frontier — is letting AI *act*, treating it as an agent.

> **Agent**: a system that can leverage (or emulate) reasoning (or equivalent processes) to make dynamic decisions in an application flow.[^1]

It’s one thing to recommend a flight. It’s another to book it.

That’s not bead-moving. That’s baton-passing.

And batons are harder. They require trust. Handoff. Real-world consequences.

To pass the baton, the model has to do more than make a choice. It needs to *run the search* in real time. *Read the API docs.* *Format the request correctly.* *Trigger the transaction.* That’s not storytelling — that’s execution.

But the model can’t do that on its own.

Why? Because tools — like flight APIs — aren’t built into its brain. They're separate. And using them takes coordination. Someone has to wire things up so that what the model *says* it wants to do can actually be *done*.

Right now, that someone is a developer.

They write the glue code. Inject the prompts. Stitch together services. They make sure the system can act on the model’s intent — not just in the story it’s telling, but in the real world.

And let’s be clear: if there’s a program, there’s a programmer.

LLMs can help — they can assist with handoffs, suggest actions, even fill in gaps — but they can’t own the whole process. Not yet. The human has to handle the parts the model isn’t ready to manage.

It works. But it’s fragile. Expensive. High-overhead.

## 🤖 Toward Agentic Infrastructure

This is where we’re heading next.

We’re standing at the edge of something big — a future where the baton can be passed *without* a developer in the loop. Where the AI isn’t just reading the guidebook, but knows when and how to act on it. Where the infrastructure exists to let a brain in a jar take action *safely*, *modularly*, *on brand*.

That’s what the next chapter explores: a generic infrastructure that exposes reusable tools that the model can employ directly on our behalf.

We’re not there yet.

But the right facilities make possible what has long been far fetched.

[^1]: This definition is brought to you by [AI Makerspace](https://www.youtube.com/watch?v=lfEbzQFyS9U).
