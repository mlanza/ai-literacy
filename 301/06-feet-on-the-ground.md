# Feet on the Ground

In the last chapter, we described the LLMâ€™s world as a kind of text adventure â€” a Zork-style simulation where the AI navigates rooms, encounters objects, and performs actions by typing verbs. But we also made a subtle shift: those objects arenâ€™t just fictional. They're props with wiring. And when wired through MCP, they become portals to the real world â€” turning fictional acts into consequential ones.

If thereâ€™s a phone in the room, someone built it. If thereâ€™s a laptop with a browser open, someone made sure it knows how to behave. And if thereâ€™s a travel agency with an NPC outside, someone encoded its rules of engagement. Behind every meaningful object is a developer who breathed life into it â€” not to simulate reality, but to expose capabilities.

This isnâ€™t light work. Every virtual prop that does something real represents an integration lift. Thatâ€™s why we aim to reuse them, wrap them, and distribute them in ways that make them easy to plug in â€” often using something like MCP. The goal is to let the model move fluidly through a world and employ tools, and hopefully not have to implement them all ourselves.

So letâ€™s see that in action. Not as a concept, but as a moment.

## ğŸš— The Scenario: â€œLet the 2pm Meeting Know Iâ€™m Running Lateâ€

Itâ€™s 1:52 PM. The executive is stuck in traffic. He says:

> â€œHey Assist, let the 2pm meeting know Iâ€™m running 10 minutes late.â€

Thatâ€™s the intention. Itâ€™s not a full plan, not a sequence of steps â€” just a quick human ask.

From there, the baton passes to the app. It spins up the model, hands it a few consequential props â€” a calendar, an address book, a texting interface â€” and lets the model take the first step.

## ğŸŒ€ Loop 1 â€” Calendar Query

The first prompt includes the userâ€™s request, transcribed from voice and sent into the app. The current time and available tools are in scope.

The app delivers this to the model:

```
You are an assistant. The user just said:
â€œLet the 2pm meeting know Iâ€™m running 10 minutes late.â€
It is currently 1:52 PM.
You may use available tools to look up calendar events.
```

The model anchors immediately on the calendar. To notify anyone, it first needs to understand what the 2pm meeting is â€” and whoâ€™s attending.

```
To notify the attendees, I need to check the userâ€™s calendar for the 2:00 PM meeting.

{
  "tool_request": {
    "tool": "calendar.query",
    "start_time": "2025-08-03T14:00:00",
    "end_time": "2025-08-03T14:30:00"
  }
}
```

This response has two layers: the natural-language explanation for the human and the structured tool call for the system. That duality is exactly what makes the LLM a **universal mediator** â€” one foot in each world, translating human intention into machine action.

The developer extracts the structured request and relays it to the MCP calendar service, which responds:

```json
{
  "tool_response": {
    "tool": "calendar.query",
    "results": [
      {
        "id": "evt-88aa9b72",
        "title": "Project Sync",
        "start_time": "2025-08-03T14:00:00",
        "end_time": "2025-08-03T14:30:00",
        "location": "Zoom",
        "attendees": [
          { "name": "Ava Bennett", "email": "ava@example.com" },
          { "name": "Marcus Liu", "email": "marcus@example.com" },
          { "name": "Leo Vargas", "email": "leo@example.com" }
        ]
      }
    ]
  }
}
```

The app injects that fact into the next prompt:

```
The 2pm meeting is titled â€œProject Syncâ€. The attendees are Ava Bennett, Marcus Liu, and Leo Vargas.
```

No instructions needed. The model already knows what tools are available and how to use them. Thatâ€™s baked into the system prompt. The app just supplies new facts â€” like a user feeding context to ChatGPT â€” trusting the model to keep the thread going.

## ğŸŒ€ Loop 2 â€” Address Book Lookup

The model immediately advances with the next logical step: resolve names into numbers.

```
To send a message, I need their mobile numbers.

{
  "tool_request": {
    "tool": "address_book.lookup",
    "names": ["Ava Bennett", "Marcus Liu", "Leo Vargas"]
  }
}
```

The app relays the request to the MCP contact service, which replies:

```json
{
  "tool_response": {
    "tool": "address_book.lookup",
    "results": [
      {
        "name": "Ava Bennett",
        "phone": "+1-555-123-0192",
        "email": "ava@example.com"
      },
      {
        "name": "Marcus Liu",
        "phone": "+1-555-388-4421",
        "email": "marcus@example.com"
      },
      {
        "name": "Leo Vargas",
        "phone": "+1-555-207-3988",
        "email": "leo@example.com"
      }
    ]
  }
}
```

As before, we donâ€™t extract or interpret â€” just pass it along. The model continues the chain.

## ğŸŒ€ Loop 3 â€” Text Message Delivery

With everything in hand, the model responds:

```
Notifying the team.

{
  "tool_request": {
    "tool": "text_message",
    "to": [
      "+1-555-123-0192",
      "+1-555-388-4421",
      "+1-555-207-3988"
    ],
    "message": "Hey everyone â€” running about 10 minutes late. Please feel free to get started without me."
  }
}
```

The app routes that to the MCP texting service, which replies:

```json
{
  "tool_response": {
    "tool": "text_message",
    "status": "sent",
    "timestamp": "2025-08-03T13:53:12Z",
    "message_id": "msg-0093442"
  }
}
```

And finally, the app confirms:

> â€œMessage sent to your 2pm meeting.â€

---

Each loop was a single LLM turn. Every tool call was surfaced explicitly â€” no black boxes, no shell commands, no behind-the-scenes orchestration.

* First, the model queried the calendar
* Then, it got attendee contact info
* Finally, it sent a message

All without a prewritten playbook. The model knew what tools were on the desk and how to use them. Likewise, the developer knew because he is the one who set the stage on which the model would perform.

However, for the model to propel itself forward in the story, I like to think of it like Link (the LLM) on a skateboard. He puts his foot down and pushes off, propelling himself forward â€” but losing momentum quickly. So he has to push again. And again. Each of those pushes? That's a model turn, happening inside the app's control loop.

That's where the developer is, right there â€” with the bead. Not choreographing the motion, but present in the loop. Watching each push. Shaping the environment.

Every tool call is routed to the MCP server. Thatâ€™s the baton pass. The developer sees what the model is trying to do and decides what gets through â€” not line-by-line, but by designing the set, laying out the props, and observing the turns.

And what gets through isnâ€™t some vague intent. Itâ€™s a sequence of clear, Zork-style moves â€” compact, consequential, and context-aware. Not the original â€œlet them know Iâ€™m running late,â€ but something more like:

```
> check calendar between ... and ...
> get attendees for â€œ...â€
> look up numbers for ..., ..., and ...
> send group text: â€œ...â€
```

Each is a turn. A push of the foot. Thatâ€™s what the app sees. Thatâ€™s what the developer vets, because he can't necessarily anticipate the intent, but he does have control over what is permitted inside the control loop.

He doesnâ€™t have to know where the story is going. He just has to decide whether this particular combination of verbs â€” on this particular stage â€” seems reasonable enough to let the action proceed.

Some developers choose to make the modelâ€™s steps visible. They show its thoughts mid-move, using chain-of-thought as a UI convention â€” a running log of reasoning that users can watch unfold.

And if you wanted to have some fun, you could sketch a little GUI. A calendar shop. A contact booth. A messaging kiosk. The model â€” Link â€” darts from place to place, completing the task. You see where it goes. You see whatâ€™s in bounds. The work becomes legible.

Most workflows arenâ€™t like that. Theyâ€™re not towns. Theyâ€™re trails. Directed. Linear. A bead moving cup to cup.

Some of those trails are hand-designed. Others are walked freely. What matters is that the bead advances through stages â€” and that each stage creates context for the next.

Thatâ€™s the pattern behind multistep features like ChatGPTâ€™s â€œdeep research.â€ Not a single move, but a control loop. The model pushes, then waits. Pushes again. Each time, shaped by whatâ€™s come before.

Because real work doesnâ€™t happen in a single turn. The facts donâ€™t arrive all at once. The structure doesnâ€™t spring up whole. Itâ€™s layered. Step by step.

Thatâ€™s why developers build systems. Why they expose waypoints. Why they separate concerns across turns.

Because structure improves output.

Because each step leaves behind byproducts.

Because serious work is iterative.

But hereâ€™s what changes once the model starts pushing â€” when it keeps moving, turn after turn. Weâ€™re no longer just talking about intelligence.

Weâ€™re talking about agency.

And that raises the real question: how far do we want it to go?

Is this a guided trail â€” tightly bounded, low-risk?

Is it a town â€” open-ended, permissive, built for discovery?

Or is it something else?

The answer depends on what weâ€™re willing to hand off â€” and how closely weâ€™re willing to stay in the loop.
