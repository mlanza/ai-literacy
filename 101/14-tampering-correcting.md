# Tampering or Correcting?

When you prompt a chatbot, you’re sending a message.

That much is obvious.

What’s not always obvious is what that message includes.

It’s not just the text you type or paste in.
If you upload a file — like an attachment in email — that gets bundled in too.
But there's more.

Tucked invisibly into every message is something called a **system prompt**. This is a background directive, often set in your ChatGPT system settings. You won’t see it, but it’s there, riding along with your input — like a sealed envelope slipped into the packet.

And that’s only the part you *can* influence.

App providers can add their own instructions. For example:

> *Always be respectful. Never use coarse language.*

That’s not tampering — not necessarily. It’s more like safety glass. The model wasn’t trained in a perfectly clean environment. It absorbed everything — brilliance and bile, nuance and nastiness. So these quiet prompts help the model respond more responsibly, smoothing over the worst edges.

But it’s a reminder: the actual context the model sees isn’t always fully visible to you. Some parts are yours. Some parts aren’t. And that boundary matters.

Depending on your perspective, what’s added can feel like help — or like interference.
That’s where tampering and correcting blur.

This dynamic shows up earlier too, during **training**.

Large language models are trained on mountains of text — books, blogs, transcripts, documentation, Wikipedia, Reddit. What gets included, how it gets weighted, and what gets excluded are all human choices.

If you decide Wikipedia is trustworthy and 4chan isn’t, you give more weight to the former.
That doesn’t mean the model believes Wikipedia. It means the probabilities shift — certain ideas become more likely to surface.

That’s not the model doing the shaping. That’s people.

Models don’t decide what matters.
They reflect what they were shaped to prioritize.

And when you step back far enough, you realize that shaping never really stops.
Not during training. Not during prompting. Not even during inference.

What you see is never just the raw model. It’s the model as wrapped, guided, filtered — nudged.

Whether that’s tampering or correcting?

That depends on who’s doing it. And why.
