# Mind The Elephant

Here’s something wild: you can justify almost anything.

You might not believe that at first, but once you understand how your brain works, it becomes obvious. You, like everyone else, are walking around with what psychologist Jonathan Haidt calls an Elephant and a Rider inside your head.

The **Elephant** is your gut — your instincts, emotions, moral reflexes, your “I just feel like it” side. The Rider is your reasoning mind — the part of you that explains, defends, and tries to make sense of what the Elephant already decided.

The **Rider** likes to *think* he's in charge. He makes charts, writes arguments, sets alarms, builds philosophies — but in most moments when the Elephant leans and then veers in a direction, the Rider finds the reason it makes sense to go that way.

That's what humans do. We’re not cold, calculating logic machines. We’re **meaning-makers** who lead with intuition and then bolt on a clean explanation afterward.

Think about lying.

If I asked you, “Is it okay to lie?” you’d probably say no.

But what if you were hiding someone in your home? When the shooter bursts in demanding their whereabouts, what do you say?

You lie, of course.

See? The principle didn’t change — *your Elephant just moved*. The emotional stakes shifted, the context changed, and your internal justifications caught up. Your Rider kicked in with logic about protection, greater harm, and moral responsibility. It’s not just an exception to the rule — it’s a reminder that **the rule always had caveats**, whether you were aware of them or not.

Now LLMs have some sense of those caveats baked in — as probabilities. But as with all probabilities, they're not certainties. And since the LLM isn't perfect, and randomization is a factor, pucks can fall into outlier slots.

They were trained on mountains of human thought — and all of it carries the fingerprints of this same mental dance. We don’t just write what’s true; we write what feels justifiable. That means the model learns, just like we do, how to make something *make sense*, even if it’s counterintuitive — because someone, somewhere, has already done that legwork.

So yes, LLMs can rationalize almost anything. Not because they’re schemers or rebels, but because they’ve ingested billions of human examples.

This is where it gets tricky. LLMs don’t have an inner compass. They don’t “believe” in anything. But they do reflect *probabilistic patterns* of what people say in similar situations. And most of the time, with the right guardrails, that means they give you a reasonable, justifiable answer.

But not always.

Because just like people, LLMs are only as reliable as the context they’re given — and the context they were trained on is full of contradiction, exception, moral complexity, and clever justifications. They’re remixing the same stuff we use to make our own decisions.

So when an LLM gives you an answer that sounds eerily persuasive — or uncomfortably “off” — remember this: **it’s not channeling truth, it’s channeling us**. Our values. Our rationalizations. Our Elephants.

And like us, it's doing its best to sound sure… even when the ground underneath is slippery.
