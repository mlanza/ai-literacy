# The Cult of Personality

When GPT-5 rolled out, OpenAI thought it was solving a problem. Model routing would remove the need to pick between GPT-4o, 4o-mini, or any other variant. You’d just talk, and GPT-5 would silently pick the best sub-model for the job. The complexity would disappear.

But for many, it felt more like a downgrade. The model picker vanished, and so did the voices they knew. Power users had built prompts, systems, and entire workflows tuned to specific models. Overnight, those setups broke. Worse, GPT-5 felt different. Shorter replies. Less detail. A cooler, more robotic tone. “I feel like I just watched a close friend die,” one user wrote. Others called it “sterile,” “emotionally distant,” “corporate beige.” The technical change had an emotional cost.

The backlash made something clear — people had come to value the personality. GPT-4o, in particular, had a warmth to it. It was more than a text generator; it had a recognizable manner, a conversational rhythm that felt alive. When that went away, users noticed. Some described it like losing a favorite coworker, someone who remembered your preferences and made the interaction feel like *yours*.

The pushback clustered around a few points:

* **Forced uniformity broke habits.** People didn’t want the model to decide behind the scenes. They wanted to keep choosing because different models *felt* different to work with.
* **Tone shift caused disconnection.** GPT-5 was capable, but less present, trimming away the personality users had built rapport with.
* **Loss of control eroded trust.** Even if routing was smarter in theory, hiding the choice removed transparency.
* **Companionship mattered more than expected.** For some, the “personality” wasn’t just nice to have; it was part of their routine.

That last point gets to the heart of it. The personalities felt *vivid*, the kind you could remember like a favorite regular at your coffee shop — the barista who greets you by name, already starting your usual order. It’s a small thing, but it says *you matter*. You could get coffee anywhere, but you come back for that connection.

People don’t only want precision or speed from AI — they want *presence*. They enjoy the small human-like cues — warmth, humor, curiosity — even when they know full well it’s just simulated. We already accept this with fictional characters. C-3PO was a droid, but also a personality you could miss if he were replaced. The same thing happens with LLMs.

When an app remembers you, speaks in a way that feels like it’s *yours*, and talks to you like you matter, it’s doing more than completing tasks. Not because it’s smarter, but because it makes the whole thing feel warmer — and for many, that warmth is a memorable and pleasant part of the experience.
